{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z9l7S6RV8-2"
      },
      "source": [
        "#### **Welcome to Assignment 4 on Deep Learning for Computer Vision.**\n",
        "In this assignment you will get a chance to implement LSTM cell from Scratch and Usage of Recurrent Neural Network for 1D time series Prediction task .\n",
        "\n",
        "#### **Instructions**\n",
        "1. Use Python 3.x to run this notebook\n",
        "3. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
        "you should not change anything else code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
        "4. Read documentation of each function carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT8skzfOLu91"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "Given a sequence of values of a 1D input time series from time t = 1 to t = 5, predict the value of the time series at t = 6 using RNN.\n",
        "\n",
        "Here we trained an RNN in such a way that, given values of input time series from t = 1 to t=i ; it will predict the value at t= i+1.\n",
        "\n",
        "Hint : Design an RNN using pytorch's nn.RNN to create an RNN layer , then add a fully-connected layer to get the required output size.\n",
        "\n",
        "Choose 32 as the number of features in the RNN output and in the hidden state. Also, choose number of layers to be 1 to make up the RNN, typically such number varies depending on different tasks. The value greater than 1 means that you'll create a stacked RNN. Also, use \"batch_first =True\". Here, \"batch_first\" implies whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim). \n",
        "\n",
        "Which of the following options is True ?\n",
        "\n",
        "1.   -2.3214\n",
        "2.   -2.2879\n",
        "3.   -2.3118\n",
        "4.   -2.2993\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVHp8ZWqYpd",
        "outputId": "43b2aa61-53fd-47a1-dc86-3b10306a53e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "## Fixing the seed for Reproducibility\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "## Define 1D input time series, which spans from t= 1 to t=6.\n",
        "input_series = np.random.randn(6,1)\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        self.rnn_ = nn.RNN(input_size=input_size,hidden_size=hidden_dim,num_layers=n_layers,batch_first=True)\n",
        "        self.linear_ = nn.Linear(in_features=hidden_dim,out_features=output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        out, hid = self.rnn_(x,hidden)\n",
        "        pred = self.linear_(out)\n",
        "        return pred,hid\n",
        "        \n",
        "        \n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "# decide on hyperparameters\n",
        "input_size=1    ## 1D input\n",
        "output_size=1   ## 1D output\n",
        "hidden_dim=32  ## Hidden state feature dimension of RNN\n",
        "n_layers=1     ## No. of stacked layers in RNN\n",
        "\n",
        "# instantiate an RNN\n",
        "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
        "\n",
        "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
        "\n",
        "# train the RNN\n",
        "def train(rnn, n_steps, print_every):\n",
        "    \n",
        "    # initialize the hidden state\n",
        "    hidden = None      \n",
        "    \n",
        "    for batch_i, step in enumerate(range(n_steps)):\n",
        "        # defining the training data \n",
        "        x = input_series[:-1]\n",
        "        y = input_series[1:]\n",
        "        \n",
        "        # convert data into Tensors\n",
        "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
        "        y_tensor = torch.Tensor(y)\n",
        "\n",
        "        # outputs from the rnn\n",
        "        prediction, hidden = rnn(x_tensor, hidden)\n",
        "\n",
        "        ## Representing Memory ##\n",
        "        # make a new variable for hidden and detach the hidden state from its history\n",
        "        # this way, we don't backpropagate through the entire history\n",
        "        hidden = hidden.data\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(prediction, y_tensor)\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # perform backprop and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # display loss and predictions\n",
        "        if batch_i%print_every == 0:  \n",
        "            print (batch_i)      \n",
        "            print('Loss: ', loss.item())\n",
        "            print ('Predicted Value: ', prediction.data.numpy().flatten())\n",
        "            print ('True Value: ', y_tensor.data.numpy().flatten())\n",
        "            \n",
        "    \n",
        "    return rnn,prediction.data.squeeze(0)[-1]\n",
        "\n",
        "# train the rnn and monitor results\n",
        "trained_rnn,final_prediction = train(rnn, n_steps = 75, print_every= 11)\n",
        "print ('Final predicted value of input time series at t=6: ',final_prediction.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Loss:  1.5117348432540894\n",
            "Predicted Value:  [-0.18635052 -0.00746429  0.05009796  0.05678663 -0.02306959]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "11\n",
            "Loss:  0.13453085720539093\n",
            "Predicted Value:  [ 0.00849465 -0.31584215 -1.2586136   1.2727585  -2.5075977 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "22\n",
            "Loss:  0.19528241455554962\n",
            "Predicted Value:  [-1.2275372  -0.65807253 -0.54608935  0.45658708 -1.9332781 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "33\n",
            "Loss:  0.08917398750782013\n",
            "Predicted Value:  [-0.07242134 -0.87209177 -1.0512494   0.82086146 -2.486608  ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "44\n",
            "Loss:  0.03353007882833481\n",
            "Predicted Value:  [-0.8939355  -0.42941052 -1.1992264   0.8064029  -2.0589507 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "55\n",
            "Loss:  0.009381239302456379\n",
            "Predicted Value:  [-0.5264543  -0.61008173 -0.9821837   0.92798245 -2.4456325 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "66\n",
            "Loss:  0.0058446116745471954\n",
            "Predicted Value:  [-0.6522237  -0.45216995 -1.0666145   0.7658678  -2.1926322 ]\n",
            "True Value:  [-0.6117564  -0.5281718  -1.0729686   0.86540765 -2.3015387 ]\n",
            "Final predicted value of input time series at t=6:  -2.3214540481567383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([5, 1])) that is different to the input size (torch.Size([1, 5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zfAVRvIFjxh"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Given a Multivariate input time sequence and all the trainable parameters of LSTM Cell; Implement all the functionalities of the LSTM cell in order to predict the hidden state and output at time=t; given LSTM \"cell state\" at previous time step (t= t-1), LSTM \"hidden state\" at previous time step ( t= t-1) and the input at time=t. Hint : Follow the following sets of equation for implementing the functionality of LSTM Cell.\n",
        "\n",
        "Forget GATE: $f_{t} = \\sigma(W_{f}[ a_{t-1} ; x_{t}]  + b_{f}) $ (Note that \";\" denotes contatenation operation.)\n",
        "\n",
        "Update GATE: $i_{t} = \\sigma(W_{i}[ a_{t-1} ; x_{t} ] + b_i )$\n",
        "\n",
        "Memory GATE: $\\tilde{c}_{t} = tanh(W_c[ a_{t-1} ; x_{t} ] + b_c )$\n",
        "            update step -> $c_{t} =  f_{t} * c_{t-1} + i_{t} * \\tilde{c}_{t}$  (This operation determines how much information to keep from past and how much to add from current step information)\n",
        "\n",
        "Output GATE: $o_{t} = \\sigma(W_o [ a_{t-1} ; x_{t} ] + b_o)$\n",
        "           Final Output: $a_{t} = o_{t}*tanh(c_t) $\n",
        "( Note: For implementing \"tanh\" operation; use numpy.tanh libary function)\n",
        "\n",
        "\n",
        "Compute the value of a specific component of LSTM cell \"Output\" (y), i.e. y[1, 3, 4]? \n",
        "\n",
        "Which of the following options is true for the value of y[1, 3, 4]?\n",
        "\n",
        "\n",
        "1.   0.1483\n",
        "2.   0.1724\n",
        "3.   0.2108\n",
        "4.   0.2471\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOok6KSeaJ1o",
        "outputId": "53685999-538b-4c13-fc20-952ee4725603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "np.random.seed(2)\n",
        "\n",
        "## Function implements Sigmoid Activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "## Function implements Softmax Activation\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "## Function implements LSTM \"forward pass\" of a single time step..i.e. given x at time step t, hidden state \n",
        "##at previous time step Memory state at previous time step , this function computes predicted output y at time step t. \n",
        "\n",
        "def lstm_forward_pass(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell \n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\"\n",
        "    a_prev -- Hidden state at timestep \"t-1\"\n",
        "    c_prev -- Memory state at timestep \"t-1\"\n",
        "\n",
        "    # Trainable Parameters of a LSTM cell\n",
        "    Wf -- Weight matrix of the forget gate; bf -- Bias of the forget gate\n",
        "    Wi -- Weight matrix of the update gate; bi -- Bias of the update gate\n",
        "    Wc -- Weight matrix of the first \"tanh\"; bc --  Bias of the first \"tanh\"\n",
        "    Wo -- Weight matrix of the output gate; bo --  Bias of the output gate\n",
        "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
        "                        \n",
        "    The LSTM Cell MUST Returns:\n",
        "    a_next -- next hidden state\n",
        "    c_next -- next memory state\n",
        "    yt_pred -- LSTM output prediction at timestep \"t\"\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the memory value\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"]; bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"]; bi = parameters[\"bi\"]\n",
        "    Wc = parameters[\"Wc\"]; bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"]; bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"]; by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### YOUR CODE STARTS HERE ###\n",
        "    ft = sigmoid(np.matmul(Wf,np.concatenate((a_prev,xt)))+bf)\n",
        "    it = sigmoid(np.matmul(Wi,np.concatenate((a_prev,xt)))+bi)\n",
        "    cct = np.tanh(np.matmul(Wc,np.concatenate((a_prev,xt)))+bc)\n",
        "    c_next = ft*c_prev + it*cct\n",
        "    ot = sigmoid(np.matmul(Wo,np.concatenate((a_prev,xt)))+bo)\n",
        "    a_next = ot * np.tanh(c_next)\n",
        "    yt_pred = softmax(np.matmul(Wy,a_next)+by)\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step\n",
        "    a0 -- Initial hidden state of LSTM cell\n",
        "    parameters \n",
        "    Wf -- Weight matrix of the forget gate ;bf -- Bias of the forget gate\n",
        "    Wi -- Weight matrix of the update gate ;bi -- Bias of the update gate\n",
        "    Wc -- Weight matrix of the first \"tanh\";bc -- Bias of the first \"tanh\"\n",
        "    Wo -- Weight matrix of the output gate; bo -- Bias of the output gate\n",
        "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
        "                        \n",
        "    This Function call MUST Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    c -- Memory states for every time-step\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### YOUR CODE STARTS HERE ###\n",
        "    # time_steps = x.shape[-1]\n",
        "    # a = []\n",
        "    # c = []\n",
        "    # y = []\n",
        "    # a_prev = a0\n",
        "    # c_prev = a0\n",
        "    # for i in range(time_steps):\n",
        "    #     a_prev, c_prev, yt, cache = lstm_forward_pass(x[:,:,i],a_prev,c_prev,parameters)\n",
        "    #     a.append(a_prev)\n",
        "    #     c.append(c_prev)\n",
        "    #     y.append(yt)\n",
        "    #     caches.append(cache)\n",
        "    # a = np.array(a)\n",
        "    # c = np.array(c)\n",
        "    # y = np.array(y)\n",
        "\n",
        "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters['Wy'].shape\n",
        "    \n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
        "    a = np.zeros([n_a, m, T_x])\n",
        "    c = np.zeros([n_a, m, T_x])\n",
        "    y = np.zeros([n_y, m, T_x])\n",
        "    \n",
        "    # Initialize a_next and c_next (≈2 lines)\n",
        "    a_next = a0\n",
        "    c_next = np.zeros([n_a,m])\n",
        "    \n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
        "        a_next, c_next, yt, cache = lstm_forward_pass(x[:,:,t], a_next, c_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "        a[:,:,t] = a_next\n",
        "        # Save the value of the prediction in y (≈1 line)\n",
        "        y[:,:,t] = yt\n",
        "        # Save the value of the next cell state (≈1 line)\n",
        "        c[:,:,t]  = c_next\n",
        "        # Append the cache into caches (≈1 line)\n",
        "        caches.append(cache)\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "\n",
        "    return a, y, c, caches\n",
        "\n",
        "# Input time sequence\n",
        "x = np.random.randn(3,10,7)\n",
        "# 3 = feature vector length\n",
        "# 10 = time-step\n",
        "# 7 = examples\n",
        "\n",
        "# Initial Hidden state of LSTM\n",
        "a0 = np.random.randn(5,10)\n",
        "\n",
        "# Weight and Bias Parameters of FORGET gate\n",
        "Weight_f = np.random.randn(5, 8); bias_f = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of UPDATE gate\n",
        "Weight_i = np.random.randn(5, 8); bias_i = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of OUTPUT gate\n",
        "Weight_o = np.random.randn(5, 8); bias_o = np.random.randn(5,1)\n",
        "\n",
        "# Weight and Bias Parameters of MEMORY gate (updating the cell)\n",
        "Weight_c = np.random.randn(5, 8); bias_c = np.random.randn(5,1)\n",
        "\n",
        "# Weight and bias for transforming hidden state output to final LSTM output for downstream application\n",
        "Weight_y = np.random.randn(2,5); bias_y = np.random.randn(2,1)\n",
        "\n",
        "LSTM_parameters = {\"Wf\": Weight_f, \"Wi\": Weight_i, \"Wo\": Weight_o, \"Wc\": Weight_c, \"Wy\": Weight_y, \"bf\": bias_f, \"bi\": bias_i, \"bo\": bias_o, \"bc\": bias_c, \"by\": bias_y}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, LSTM_parameters)\n",
        "\n",
        "## Print the specific component value of LSTM cell \"Output\" (y) ;i.e. y[1,3,4]\n",
        "print(\"y[1][3][4] =\", y[1][3][4])\n",
        "print(\"y.shape = \", y.shape)\n",
        "\n",
        "## Print the specific component value of LSTM \"hidden state\" Output (a) ;i.e. a[2,1,5]\n",
        "print(\"a[2][1][5] = \", a[2][1][5])\n",
        "print(\"a.shape = \", a.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y[1][3][4] = 0.21083866421151456\n",
            "y.shape =  (2, 10, 7)\n",
            "a[2][1][5] =  0.023019435434183624\n",
            "a.shape =  (5, 10, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SQRxSU9Tkwx"
      },
      "source": [
        "## Question 3:\n",
        "\n",
        "Using the exact same code as given for question 2, find the value of a specific component of LSTM hidden state output(a) ; i.e. a[2,1,5].\n",
        "\n",
        "Which of the values is true for a[2, 1, 5]?\n",
        "\n",
        "\n",
        "\n",
        "1.   0.0140\n",
        "2.   0.0230\n",
        "3.   0.0272\n",
        "4.   0.0312\n",
        "\n",
        "\n"
      ]
    }
  ]
}