{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4V Assignment 6 (Week 12)- Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEpvOceUf0lE"
      },
      "source": [
        "#### **Welcome to Assignment 6 on Deep Learning for Computer Vision.**\n",
        "In this assignment you will get a chance to implement Projected Gradient Descent and Rotation Based Self Supervised Learning Technique .\n",
        "\n",
        "#### **Instructions**\n",
        "1. Use Python 3.x to run this notebook\n",
        "3. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
        "you sould not change anything else code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
        "4. Read documentation of each function carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzaL9YMibBNB"
      },
      "source": [
        "### Question 1 : Implement Projected Gradient Descent\n",
        "\n",
        "Given a sample test image and a pretrained model, generate a corresponding adversarial image using Projected gradient Descent(PGD). The following attack configuration MUST be follwed in order to generate the adversarial image: step size = 2/255, epsilon = 0.3 and number_of_steps = 40. \n",
        " \n",
        "Find out the predicted class when the adversarial image generated in the previous step is fed to the pretrained model?\n",
        "\n",
        "\n",
        "\n",
        "1.   Predicted class: 3\n",
        "2.   Predicted class: 4\n",
        "3.   Predicted class: 5\n",
        "4.   Predicted class: 6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlqzGiwscpb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "d5db762f-5ca0-4522-c3ae-352442f0fe46"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "epsilons = 0.3\n",
        "pretrained_model = \"/content/lenet_mnist_model.pth\"\n",
        "use_cuda=True\n",
        "\n",
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        #return F.log_softmax(x, dim=1)\n",
        "\n",
        "# MNIST Test dataset and dataloader declaration\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            ])), \n",
        "        batch_size=1, shuffle=True)\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "# Initialize the network\n",
        "model = Net().to(device)\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
        "\n",
        "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "model.eval()\n",
        "\n",
        "## Implement Projected Gradient Descent algorithm\n",
        "\n",
        "### YOUR CODE STARTS HERE\n",
        "def PGD_attack(data, target, model):\n",
        "        images = data\n",
        "        labels = target\n",
        "        iter = 40; eps = 0.3; alpha = 2 / 255 ;\n",
        "        criterion_ce = nn.CrossEntropyLoss()\n",
        "        ori_images = images.clone().detach()\n",
        "        \n",
        "        for i in range(40) :    \n",
        "            images.requires_grad = True\n",
        "            preds = model(images)\n",
        "            cost = criterion_ce(preds,labels) \n",
        "\n",
        "            grad = torch.autograd.grad(cost, images, \n",
        "                                       retain_graph=False, create_graph=False)[0]\n",
        "\n",
        "            adv_images = images + alpha*grad.sign()\n",
        "            eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
        "            images = torch.clamp(ori_images + eta, min=int(torch.min(ori_images)), max=int(torch.max(ori_images))).detach()\n",
        "\n",
        "        adv_images = images\n",
        "        \n",
        "        return adv_images \n",
        "\n",
        "### YOUR CODE ENDS HERE\n",
        "\n",
        "def test( model, device, data, target, epsilon ):\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    # Send the data and label to the device\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Set requires_grad attribute of tensor. Important for Attack\n",
        "    data.requires_grad = True\n",
        "\n",
        "    # Forward pass the data through the model\n",
        "    output = model(data)\n",
        "    init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "    ### generate the perturbed image using PGD    \n",
        "    perturbed_data = PGD_attack(data, target, model)\n",
        "        \n",
        "    # Re-classify the perturbed image\n",
        "    output = model(perturbed_data)\n",
        "\n",
        "    # Check for success\n",
        "    final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "    if final_pred.item() == target.item():\n",
        "        correct += 1\n",
        "    else:\n",
        "        pass\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_pred, perturbed_data\n",
        "\n",
        "for data,target, in test_loader:\n",
        "  data = data[0:1,:,:,:]\n",
        "  target = target[0:1]\n",
        "  break\n",
        "\n",
        "\n",
        "pred_adv, adv_ex = test(model, device, data,target, epsilons)\n",
        "print (\"Predicted class for perturbed image: \",pred_adv)\n",
        "\n",
        "### YOUR CODE STARTS HERE\n",
        "\n",
        "## Compute mean pixel value of the perturbed image\n",
        "ex = adv_ex.squeeze().detach().cpu().numpy()\n",
        "z = np.mean(ex)\n",
        "print (\"Mean pixel intensity of perturbed image: \",z)\n",
        "\n",
        "## Visualize the perturbed image\n",
        "plt.imshow(ex, cmap=\"gray\")\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n",
            "Predicted class for perturbed image:  tensor([[5]])\n",
            "Mean pixel intensity of perturbed image:  0.19408265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3661f39be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abm7O-0ngxJC"
      },
      "source": [
        "### Question 2 : Visualize The adversarial image generated  using the exactly same setup as in previous question and find out the mean pixel intensity of that adversarial image?\n",
        "\n",
        "\n",
        "1.   0.1570\n",
        "2.   0.1940\n",
        "3.   0.2170\n",
        "4.   0.2390\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb8rWSyTqiAU"
      },
      "source": [
        "\n",
        "Please consider the modified LeNet model with below model definition:\n",
        "\n",
        "Shared layers (1 to 4)\n",
        "\n",
        "\n",
        "1.   Conv layer with 10 output channels and filter size 5\n",
        "2.   Conv layer with 20 output channels and filter size 5\n",
        "3.   Dropout layer\n",
        "4.   Fully connected layer with output size 50\n",
        "5.   Branch out 2 heads i.e. main classification and rotation  classification heads.\n",
        "\n",
        "  *   Takes input from step 4 and outputs 10 dimensions(main class labels) through a fully connected layer\n",
        "  *   Takes input from step 4 and outputs 4 dimensions(rotation class labels) through a fully connected layer\n",
        "\n",
        " \n",
        "\n",
        "This model is basically a Y-shaped model where the trail is shared layers and 2 heads are for main classification and rotation classification. A model with above definition is trained for 20 epochs and the resulting trained model is shared with you. Also some steps in the forward function are kept blank for you to fill up. You have to load the model and properly write those steps in the forward function to be able to run the model. Please note that without these steps properly written, you won't be able to run the model. Once you do this, please answer the below question.\n",
        "\n",
        "What is the model test accuracy on MNIST test dataset?\n",
        "\n",
        "1.  92.37\n",
        "2.  93.62\n",
        "3.  94.49\n",
        "4.  95.91"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f61-AzsYotaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e99018a-99b6-48ee-aa1d-faeb66962a26"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Assumes that tensor is (nchannels, height, width)\n",
        "def tensor_rot_90(x):\n",
        "    return x.flip(2).transpose(1, 2)\n",
        "\n",
        "def tensor_rot_180(x):\n",
        "    return x.flip(2).flip(1)\n",
        "\n",
        "def tensor_rot_270(x):\n",
        "    return x.transpose(1, 2).flip(2)\n",
        "\n",
        "def rotate_batch_with_labels(batch, labels):\n",
        "\timages = []\n",
        "\tfor img, label in zip(batch, labels):\n",
        "\t\tif label == 1:\n",
        "\t\t\timg = tensor_rot_90(img)\n",
        "\t\telif label == 2:\n",
        "\t\t\timg = tensor_rot_180(img)\n",
        "\t\telif label == 3:\n",
        "\t\t\timg = tensor_rot_270(img)\n",
        "\t\timages.append(img.unsqueeze(0))\n",
        "\treturn torch.cat(images)\n",
        "\n",
        "def rotate_batch(batch, label):\n",
        "\tif label == 'rand':\n",
        "\t\tlabels = torch.randint(4, (len(batch),), dtype=torch.long)\n",
        "\telif label == 'expand':\n",
        "\t\tlabels = torch.cat([torch.zeros(len(batch), dtype=torch.long),\n",
        "\t\t\t\t\ttorch.zeros(len(batch), dtype=torch.long) + 1,\n",
        "\t\t\t\t\ttorch.zeros(len(batch), dtype=torch.long) + 2,\n",
        "\t\t\t\t\ttorch.zeros(len(batch), dtype=torch.long) + 3])\n",
        "\t\tbatch = batch.repeat((4,1,1,1))\n",
        "\telse:\n",
        "\t\tassert isinstance(label, int)\n",
        "\t\tlabels = torch.zeros((len(batch),), dtype=torch.long) + label\n",
        "\treturn rotate_batch_with_labels(batch, labels), labels\n",
        "\n",
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        self.fc2_ssl = nn.Linear(50, 4)\n",
        "\n",
        "    ### network architecture for classification head: \n",
        "    ### conv1 -> maxpool2D-> Relu->conv2->conv2_drop->maxpool2D->Relu->Reshape->fc1->Relu->dropout->fc2,fc2_ssl\n",
        "    def forward(self, x):\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        # classification head\n",
        "        out_cls = self.fc2(x)\n",
        "        # self supervised head                \n",
        "        out_ssl = self.fc2_ssl(x)\n",
        "        return out_cls, out_ssl\n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "# MNIST Test dataset and dataloader declaration\n",
        "transform = transforms.Compose([\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# the datasets\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "# Initialize the network\n",
        "net = Net().to(device)\n",
        "\n",
        "parameters = list(net.parameters())\n",
        "optimizer = optim.SGD(parameters, lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "print('Running...')\n",
        "\n",
        "def train(epoch):\n",
        "    net.train()    \n",
        "    \n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)        \n",
        "        labels_full = labels.repeat(4)            \n",
        "        \n",
        "        ## Self supervised head\n",
        "        inputs_ssh, labels_ssh = rotate_batch(inputs, \"expand\")\n",
        "        inputs_ssh, labels_ssh = inputs_ssh.to(device), labels_ssh.to(device)\n",
        "        # outputs_clh , outputs_ssh denotes classification head output and self supervision head output respectively \n",
        "        outputs_clh, outputs_ssh = net(inputs_ssh) \n",
        "        loss = criterion(outputs_clh, labels_full)\n",
        "        loss_ssh = criterion(outputs_ssh, labels_ssh)\n",
        "        loss += loss_ssh\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(('Epoch %d: %f' %(epoch,loss.item())))\n",
        "    torch.save(net, \"ssl_mnist.pt\")\n",
        "\n",
        "## Funtion to compute test accuracy using model already trained with additional self-supervised head..\n",
        "def test():\n",
        "        ### YOUR CODE STARTS HERE\n",
        "        net=torch.load(\"/content/ssl_mnist.pt\", map_location='cpu') \n",
        "        net.eval()\n",
        "        net.to(device)\n",
        "        test_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs,_ = net(inputs)\n",
        "                # loss = criterion(outputs, targets)\n",
        "\n",
        "                # test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        acc = 100.*correct/total\n",
        "        print('Test Accuracy: %f' %(acc))\n",
        "        ### YOUR CODE ENDS HERE\n",
        "\n",
        "### Training Loop\n",
        "#for epoch in range(0, 20):\n",
        "    #train(epoch)\n",
        "\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  False\n",
            "Running...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:625: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 94.490000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}