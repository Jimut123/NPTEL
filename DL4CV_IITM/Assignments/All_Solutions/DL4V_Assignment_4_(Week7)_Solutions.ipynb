{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCBjxEJ4qUnn"
   },
   "source": [
    "#### **Welcome to Assignment 3 on Deep Learning for Computer Vision.**\n",
    "This notebook consists of two parts. In Part-1 you'll have to code a Siamese Network, for Part-2 you need to go through a official PyTorch tutorial, understand it and answer some questions.\n",
    "  \n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!\n",
    "\n",
    "**Acknowledgement:** Some parts of this implementation are inspired from https://github.com/adambielski/siamese-triplet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXelHTkWTlD4"
   },
   "source": [
    "# Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJwH6jxrqI-5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import timeit\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "########################\n",
    "\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "# check availability of GPU and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#### YOUR CODE ENDS HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhSvcqdYqJ6U"
   },
   "source": [
    "#### Prepare the dataset for Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stYbGPoLqzDE"
   },
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a set of transforms for preparing the dataset\n",
    "        self.transform = transforms.Compose([\n",
    "                              transforms.ToTensor(), # convert the image to a pytorch tensor\n",
    "                              transforms.Normalize((0.137,), (0.3081,)) # normalise the images with mean and std of the dataset\n",
    "                          ])\n",
    "        # Load the MNIST training, test datasets using `torchvision.datasets.MNIST\n",
    "        # set the train parameter to self.train and transform parameter to self.transform\n",
    "        self.dataset = datasets.MNIST('./data/', train=self.train, download=True,\n",
    "                          transform=self.transform)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        if self.train:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of training data \n",
    "            self.train_data = self.dataset.train_data\n",
    "            # assign labels of training data \n",
    "            self.train_labels = self.dataset.train_labels\n",
    "            # get the set of all the labels in the dataset\n",
    "            self.labels_all = set(self.train_labels.numpy())\n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "            for each_label in self.labels_all:\n",
    "              self.label_to_idx[each_label] = np.where(self.train_labels.numpy() == each_label)[0]\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        else:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of test data \n",
    "            self.test_data = self.dataset.test_data\n",
    "            # assign labels of test data \n",
    "            self.test_labels = self.dataset.test_labels\n",
    "            # get the set of all labels in the dataset\n",
    "            self.labels_all = set(self.test_labels.numpy())\n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "            for each_label in self.labels_all:\n",
    "              self.label_to_idx[each_label] = np.where(self.test_labels.numpy() == each_label)[0]\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "            # DONOT change this line of code  \n",
    "            random_state = np.random.RandomState(0)\n",
    "\n",
    "            positive_samples = [] # this will be a list of lists\n",
    "            for ind in range(0, len(self.test_data), 2):\n",
    "              positive_samples.append([ind, random_state.choice(self.label_to_idx[self.test_labels[ind].item()]), 1])\n",
    "            \n",
    "            negative_samples = []\n",
    "            for ind in range(1, len(self.test_data), 2):\n",
    "              negative_samples.append([ind, random_state.choice(self.label_to_idx[np.random.choice(\n",
    "                                                           list(self.labels_all - set([self.test_labels[ind].item()])))]), 0])\n",
    "            \n",
    "            # combine both positive and negative samples\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            self.test_samples = positive_samples + negative_samples\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            first_image, first_label = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_idx[first_label])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_all - set([first_label])))\n",
    "                siamese_index = np.random.choice(self.label_to_idx[siamese_label])\n",
    "            second_image = self.train_data[siamese_index]\n",
    "        else:\n",
    "            first_image = self.test_data[self.test_samples[index][0]]\n",
    "            second_image = self.test_data[self.test_samples[index][1]]\n",
    "            target = self.test_samples[index][2]\n",
    "        first_image = Image.fromarray(first_image.numpy(), mode='L')\n",
    "        second_image = Image.fromarray(second_image.numpy(), mode='L')\n",
    "        first_image = self.transform(first_image)\n",
    "        second_image = self.transform(second_image)\n",
    "        return (first_image, second_image), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gEE-dEarnvg"
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # Define a sequential block as per the instructions below:\n",
    "        # Build three blocks with each block containing: Conv->PReLU->Maxpool layers\n",
    "        # Three conv layers should have 16, 32, 64 output channels respectively\n",
    "        # Use convolution kernel size 3\n",
    "        # For maxpool use a kernel size of 2 and stride of 2\n",
    "\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 16, 3), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(16, 32, 3), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 3), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),)\n",
    "        # Define linear->PReLU->linear->PReLU->linear\n",
    "        # The first two linear layers should have 256 and 128 output nodes\n",
    "        # The final FC layer should have 2 nodes\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 1 * 1, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 128),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(128, 2)\n",
    "                                )\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # Define the forward pass, convnet -> fc\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPIClNjsrz78"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Call the embedding network for both the inputs and return the output\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        op1 = self.embedding_net(x1)\n",
    "        op2 = self.embedding_net(x2)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return op1, op2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzlZzVgmM3hC"
   },
   "source": [
    "$$\n",
    "L\\left(x_{0}, x_{1}, y\\right)=\\frac{1}{2} y\\left\\|f\\left(x_{0}\\right)-f\\left(x_{1}\\right)\\right\\|_{2}^{2}+\\frac{1}{2}(1-y)\\left\\{\\max \\left(0, m-\\left\\|(f\\left(x_{0}\\right)-f\\left(x_{1}) + \\epsilon\\right)\\right\\|_{2}\\right)\\right\\}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BclsdWZSr4RK"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLossSiamese(nn.Module):\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLossSiamese, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        # Use the equation mentioned above to define the loss\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        distances = (output2 - output1).pow(2).sum(1) \n",
    "        loss_value = 0.5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        loss_value = loss_value.mean()\n",
    "\n",
    "        return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVrUkFLmca1I"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target if len(target) > 0 else None\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        # data is not a single value here,\n",
    "        # ensure datatype of variable `data` is tuple\n",
    "        data = tuple(each.to(device) for each in data)\n",
    "        target = target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named outputs\n",
    "        # python star operator will be useful here\n",
    "        # if the datatype of outputs is not a tuple, make it to a tuple\n",
    "        outputs = model(*data)\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "        loss_inputs = outputs\n",
    "        # create inputs to the contrastive loss\n",
    "        # datatype of target should be tuple\n",
    "        if target is not None:\n",
    "          target = (target,)\n",
    "        loss_inputs += target\n",
    "        # calculate the loss\n",
    "        loss = criterion(*loss_inputs)\n",
    "        # append the loss to losses list and update the total_loss variable\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses)))  \n",
    "    total_loss /= (batch_idx + 1)\n",
    "    print('Average loss on training set: {:.6f}'.format(total_loss))\n",
    "\n",
    "def test(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "          target = target if len(target) > 0 else None\n",
    "          if not type(data) in (tuple, list):\n",
    "              data = (data,)\n",
    "          #### YOUR CODE STARTS HERE ####\n",
    "          # send the image, target to the device\n",
    "          # data is not a single value here,\n",
    "          # ensure datatype of variable `data` is tuple\n",
    "          data = tuple(each.to(device) for each in data)\n",
    "          target = target.to(device)\n",
    "          # pass the image to the model and assign the output to variable named outputs\n",
    "          # python star operator will be useful here\n",
    "          # if the datatype of outputs is not a tuple, make it to a tuple\n",
    "          outputs = model(*data)\n",
    "          if type(outputs) not in (tuple, list):\n",
    "              outputs = (outputs,)\n",
    "          # create inputs to the contrastive loss\n",
    "          # datatype of target should be tuple\n",
    "          loss_inputs = outputs\n",
    "          if target is not None:\n",
    "              target = (target,)\n",
    "          loss_inputs += target\n",
    "          # calculate the loss\n",
    "          loss = criterion(*loss_inputs)\n",
    "          # update the test+loss variable\n",
    "          test_loss += loss.item()\n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Average loss on test set: {:.6f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "aDZAVVWVcAC5",
    "outputId": "7c187bc4-5ba0-47c7-9b72-027cc4bd2104"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# define the training and test sets\n",
    "# use SiameseDataset\n",
    "train_dataset = SiameseDataset(train=True)\n",
    "test_dataset = SiameseDataset(train=False)\n",
    "\n",
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 128 and set shuffle=True for the training set, set num_workers to 2 and pin_memory to True\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "margin = 1.\n",
    "# create a instance of the embedding network and pass it as input to Siamese network\n",
    "embedding_net = EmbeddingNet()\n",
    "model = SiameseNetwork(embedding_net)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# define the contrative loss with the specified margin\n",
    "criterion = ContrastiveLossSiamese(margin)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CejgunVGzJPK",
    "outputId": "af8bd0d0-a0a7-4d56-cc25-3a9fa9b8761a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.266641\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.193093\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.140615\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.121984\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.109181\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.099338\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.092354\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.086786\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.082563\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.078889\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.075679\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.072721\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.070194\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.067813\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.065780\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.063919\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.062343\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.060882\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.059441\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.058064\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.056834\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.055826\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.055036\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.053971\n",
      "Average loss on training set: 0.053572\n",
      "Average loss on test set: 0.028328\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.020969\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.035176\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.034448\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.034110\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.034688\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.032695\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.032050\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.031406\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.030816\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.030549\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.030122\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.029560\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.029034\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.028732\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.028787\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.028634\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.028314\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.028036\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.027763\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.027720\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.027598\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.027601\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.027370\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.027090\n",
      "Average loss on training set: 0.026986\n",
      "Average loss on test set: 0.020759\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.023680\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.023475\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.023150\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.023066\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.022819\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.022841\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.022880\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.023075\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.023515\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.023880\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.023983\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.024400\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.024467\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.024418\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.024333\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.024303\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.024466\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.024410\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.024491\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.024340\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.024062\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.024201\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.024320\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.024228\n",
      "Average loss on training set: 0.024230\n",
      "Average loss on test set: 0.022729\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.027481\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.024464\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.022953\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.022502\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.021606\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.021243\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.020745\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.020906\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.020396\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.020534\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.020540\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.020863\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.020980\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.021364\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.021315\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.021178\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.020888\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.020674\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.020511\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.020570\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.020886\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.021047\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.021072\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.021204\n",
      "Average loss on training set: 0.021231\n",
      "Average loss on test set: 0.026279\n",
      "Total time taken: 127 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 5):\n",
    "  train(model, train_dataloader, device, optimizer, criterion, epoch)\n",
    "  test(model, test_dataloader, device, criterion)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S0vb38a_o_r"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Run the code cell above and report the average loss on the test set loss (If you are not getting the exact number shown in options, please report the closest number).\n",
    "1. 0.03\n",
    "2. 0.3\n",
    "3. 0.001\n",
    "4. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH8VV61xTvPq"
   },
   "source": [
    "# Part-2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL4V Assignment-4 (Week7) Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
